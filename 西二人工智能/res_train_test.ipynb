{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (start): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (a1_1): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (b1_1): Sequential()\n",
      "  (a1_2): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (b1_2): Sequential()\n",
      "  (a1_3): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (b1_3): Sequential()\n",
      "  (a2_1): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (b2_1): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (a2_2): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (b2_2): Sequential()\n",
      "  (a2_3): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (b2_3): Sequential()\n",
      "  (a2_4): Sequential(\n",
      "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (b2_4): Sequential()\n",
      "  (a3_1): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (b3_1): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (a3_2): Sequential(\n",
      "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (b3_2): Sequential()\n",
      "  (a3_3): Sequential(\n",
      "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (b3_3): Sequential()\n",
      "  (a3_4): Sequential(\n",
      "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (b3_4): Sequential()\n",
      "  (a3_5): Sequential(\n",
      "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (b3_5): Sequential()\n",
      "  (a3_6): Sequential(\n",
      "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (b3_6): Sequential()\n",
      "  (a4_1): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (b4_1): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (a4_2): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (b4_2): Sequential()\n",
      "  (a4_3): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (b4_3): Sequential()\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (avg): AdaptiveAvgPool2d(output_size=1)\n",
      "  (linear): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.block = {}\n",
    "        self.start = nn.Sequential(\n",
    "            nn.Conv2d(3,64,7,2,3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3,2,1)\n",
    "        )\n",
    "        \n",
    "        self.a1_1,self.b1_1 = self.Residual(64,64,3,1)\n",
    "        self.a1_2,self.b1_2 = self.Residual(64,64,3,1)\n",
    "        self.a1_3,self.b1_3 = self.Residual(64,64,3,1)\n",
    "        \n",
    "        self.a2_1,self.b2_1 = self.Residual(64,128,3,2)\n",
    "        self.a2_2,self.b2_2 = self.Residual(128,128,3,1)\n",
    "        self.a2_3,self.b2_3 = self.Residual(128,128,3,1)\n",
    "        self.a2_4,self.b2_4 = self.Residual(128,128,3,1)\n",
    "        \n",
    "        self.a3_1,self.b3_1 = self.Residual(128,256,3,2)\n",
    "        self.a3_2,self.b3_2 = self.Residual(256,256,3,1)\n",
    "        self.a3_3,self.b3_3 = self.Residual(256,256,3,1)\n",
    "        self.a3_4,self.b3_4 = self.Residual(256,256,3,1)\n",
    "        self.a3_5,self.b3_5 = self.Residual(256,256,3,1)\n",
    "        self.a3_6,self.b3_6 = self.Residual(256,256,3,1)\n",
    "        \n",
    "        self.a4_1,self.b4_1 = self.Residual(256,512,3,2)\n",
    "        self.a4_2,self.b4_2 = self.Residual(512,512,3,1)\n",
    "        self.a4_3,self.b4_3 = self.Residual(512,512,3,1)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.avg = nn.AdaptiveAvgPool2d(1)\n",
    "        self.linear = nn.Linear(512,2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.start(x)\n",
    "        \n",
    "        x = self.Block(self.a1_1,self.b1_1,x)\n",
    "        x = self.Block(self.a1_2,self.b1_2,x)\n",
    "        x = self.Block(self.a1_3,self.b1_3,x)\n",
    "        \n",
    "        x = self.Block(self.a2_1,self.b2_1,x)\n",
    "        x = self.Block(self.a2_2,self.b2_2,x)\n",
    "        x = self.Block(self.a2_3,self.b2_3,x)\n",
    "        x = self.Block(self.a2_4,self.b2_4,x)\n",
    "        \n",
    "        x = self.Block(self.a3_1,self.b3_1,x)\n",
    "        x = self.Block(self.a3_2,self.b3_2,x)\n",
    "        x = self.Block(self.a3_3,self.b3_3,x)\n",
    "        x = self.Block(self.a3_4,self.b3_4,x)\n",
    "        x = self.Block(self.a3_5,self.b3_5,x)\n",
    "        x = self.Block(self.a3_6,self.b3_6,x)\n",
    "        \n",
    "        x = self.Block(self.a4_1,self.b4_1,x)\n",
    "        x = self.Block(self.a4_2,self.b4_2,x)\n",
    "        x = self.Block(self.a4_3,self.b4_3,x)\n",
    "        \n",
    "        x = self.avg(x)\n",
    "        \n",
    "        x = self.linear(x.view(x.size()[0],-1))\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def Residual(self,C_in,C_out,size,stride):\n",
    "            over = nn.Sequential(\n",
    "                    nn.Conv2d(C_in,C_out,size,stride,1),\n",
    "                    nn.BatchNorm2d(C_out),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(C_out,C_out,size,1,1),\n",
    "                    nn.BatchNorm2d(C_out)\n",
    "                )\n",
    "            if C_in !=C_out:   \n",
    "                change = nn.Sequential(\n",
    "                        nn.Conv2d(C_in,C_out,1,stride),\n",
    "                        nn.BatchNorm2d(C_out)\n",
    "                    )\n",
    "            else:\n",
    "                change = nn.Sequential()\n",
    "            return over,change\n",
    "        \n",
    "    def Block(self,a,b,x):\n",
    "        out = a(x)\n",
    "        x = b(x)\n",
    "        x = self.relu(x+out)\n",
    "        return x\n",
    "    \n",
    "net = Net().cuda()\n",
    "print(net)\n",
    "net.load_state_dict(torch.load('params_res_train.pkl'))\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08)\n",
    "loss_func=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 3, 64, 64]) torch.Size([500]) tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
      "        0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
      "        1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n",
      "        0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n",
      "        1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
      "        1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
      "        1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0,\n",
      "        1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0,\n",
      "        1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
      "        1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0,\n",
      "        0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0,\n",
      "        1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0,\n",
      "        1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1,\n",
      "        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "def load_dataset():\n",
    "    train_dataset=h5py.File('test_500_dataset.h5','r')\n",
    "    train_x=train_dataset['test_x'][:]\n",
    "    train_y=train_dataset['test_y'][:]\n",
    "    train_x=torch.FloatTensor(train_x)\n",
    "    train_y=torch.LongTensor(train_y)\n",
    "    return train_x,train_y\n",
    "train_x,train_y=load_dataset()\n",
    "print(train_x.size(),train_y.size(),train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0次成本:tensor(1.3089, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第10次成本:tensor(0.0279, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第20次成本:tensor(0.0118, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第30次成本:tensor(0.0016, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第40次成本:tensor(0.0004, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第50次成本:tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第60次成本:tensor(0.0002, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第70次成本:tensor(0.0001, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第80次成本:tensor(9.8915e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第90次成本:tensor(8.3478e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第100次成本:tensor(7.1945e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第110次成本:tensor(6.2902e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第120次成本:tensor(5.5585e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第130次成本:tensor(4.9534e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第140次成本:tensor(4.4443e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第150次成本:tensor(4.0092e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第160次成本:tensor(3.6348e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第170次成本:tensor(3.3111e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第180次成本:tensor(3.0272e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第190次成本:tensor(2.7781e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第200次成本:tensor(2.5578e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第210次成本:tensor(2.3627e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第220次成本:tensor(2.1879e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第230次成本:tensor(2.0317e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第240次成本:tensor(1.8909e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第250次成本:tensor(1.7650e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第260次成本:tensor(1.6499e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第270次成本:tensor(1.5471e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第280次成本:tensor(1.4528e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第290次成本:tensor(1.3647e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第300次成本:tensor(1.2868e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第310次成本:tensor(1.2152e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第320次成本:tensor(1.1492e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第330次成本:tensor(1.0878e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第340次成本:tensor(1.0312e-05, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第350次成本:tensor(9.7971e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第360次成本:tensor(9.3193e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第370次成本:tensor(8.8663e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第380次成本:tensor(8.4410e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第390次成本:tensor(8.0547e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第400次成本:tensor(7.6914e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第410次成本:tensor(7.3528e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第420次成本:tensor(7.0457e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第430次成本:tensor(6.7463e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第440次成本:tensor(6.4650e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第450次成本:tensor(6.2065e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第460次成本:tensor(5.9643e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第470次成本:tensor(5.7354e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第480次成本:tensor(5.5161e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第490次成本:tensor(5.2996e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第500次成本:tensor(5.1146e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第510次成本:tensor(4.9334e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第520次成本:tensor(4.7493e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第530次成本:tensor(4.5862e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第540次成本:tensor(4.4260e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第550次成本:tensor(4.2896e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第560次成本:tensor(4.1552e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第570次成本:tensor(4.0131e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第580次成本:tensor(3.8853e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第590次成本:tensor(3.7680e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第600次成本:tensor(3.6430e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第610次成本:tensor(3.5391e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第620次成本:tensor(3.4246e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第630次成本:tensor(3.3207e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第640次成本:tensor(3.2349e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第650次成本:tensor(3.1261e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第660次成本:tensor(3.0394e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第670次成本:tensor(2.9478e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第680次成本:tensor(2.8744e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第690次成本:tensor(2.8038e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第700次成本:tensor(2.7275e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第710次成本:tensor(2.6531e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第720次成本:tensor(2.5921e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第730次成本:tensor(2.5234e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第740次成本:tensor(2.4700e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第750次成本:tensor(2.3985e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第760次成本:tensor(2.3384e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第770次成本:tensor(2.2840e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第780次成本:tensor(2.2364e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第790次成本:tensor(2.1706e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第800次成本:tensor(2.1124e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第810次成本:tensor(2.0666e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第820次成本:tensor(2.0151e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第830次成本:tensor(1.9789e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第840次成本:tensor(1.9350e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第850次成本:tensor(1.8873e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第860次成本:tensor(1.8415e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第870次成本:tensor(1.8120e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第880次成本:tensor(1.7700e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第890次成本:tensor(1.7262e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第900次成本:tensor(1.6870e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第910次成本:tensor(1.6508e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第920次成本:tensor(1.6146e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第930次成本:tensor(1.5764e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第940次成本:tensor(1.5421e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第950次成本:tensor(1.5116e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第960次成本:tensor(1.4791e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第970次成本:tensor(1.4572e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第980次成本:tensor(1.4296e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第990次成本:tensor(1.4009e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1000次成本:tensor(1.3762e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1010次成本:tensor(1.3561e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1020次成本:tensor(1.3275e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1030次成本:tensor(1.2960e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1040次成本:tensor(1.2722e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1050次成本:tensor(1.2426e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1060次成本:tensor(1.2207e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1070次成本:tensor(1.2035e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1080次成本:tensor(1.1864e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1090次成本:tensor(1.1568e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1100次成本:tensor(1.1415e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1110次成本:tensor(1.1253e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1120次成本:tensor(1.1101e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1130次成本:tensor(1.0862e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1140次成本:tensor(1.0576e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1150次成本:tensor(1.0347e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第1160次成本:tensor(1.0223e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1170次成本:tensor(1.0061e-06, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1180次成本:tensor(9.9182e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1190次成本:tensor(9.7656e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1200次成本:tensor(9.6226e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1210次成本:tensor(9.4318e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1220次成本:tensor(9.2888e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1230次成本:tensor(9.1934e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1240次成本:tensor(9.0027e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1250次成本:tensor(8.7833e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1260次成本:tensor(8.6212e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1270次成本:tensor(8.4877e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1280次成本:tensor(8.3542e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1290次成本:tensor(8.2397e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1300次成本:tensor(8.1348e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1310次成本:tensor(8.0585e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1320次成本:tensor(7.9632e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1330次成本:tensor(7.8297e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1340次成本:tensor(7.7152e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1350次成本:tensor(7.5531e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1360次成本:tensor(7.4482e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1370次成本:tensor(7.3624e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1380次成本:tensor(7.2002e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1390次成本:tensor(7.1049e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1400次成本:tensor(7.0190e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1410次成本:tensor(6.9427e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1420次成本:tensor(6.8665e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1430次成本:tensor(6.6948e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1440次成本:tensor(6.5899e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1450次成本:tensor(6.4850e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1460次成本:tensor(6.4182e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1470次成本:tensor(6.3133e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1480次成本:tensor(6.2370e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1490次成本:tensor(6.1893e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1500次成本:tensor(6.1035e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1510次成本:tensor(5.9414e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1520次成本:tensor(5.9128e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1530次成本:tensor(5.7793e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1540次成本:tensor(5.7316e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1550次成本:tensor(5.6458e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1560次成本:tensor(5.5599e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1570次成本:tensor(5.4550e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1580次成本:tensor(5.3787e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1590次成本:tensor(5.3215e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1600次成本:tensor(5.2071e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1610次成本:tensor(5.1594e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1620次成本:tensor(5.0831e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1630次成本:tensor(5.0449e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1640次成本:tensor(5.0068e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1650次成本:tensor(4.9686e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1660次成本:tensor(4.9305e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1670次成本:tensor(4.8637e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1680次成本:tensor(4.8351e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1690次成本:tensor(4.7016e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1700次成本:tensor(4.6730e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1710次成本:tensor(4.6253e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1720次成本:tensor(4.5586e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1730次成本:tensor(4.4632e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1740次成本:tensor(4.3869e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1750次成本:tensor(4.3488e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1760次成本:tensor(4.2820e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1770次成本:tensor(4.2439e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1780次成本:tensor(4.1962e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1790次成本:tensor(4.1580e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1800次成本:tensor(4.1008e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1810次成本:tensor(4.0627e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1820次成本:tensor(4.0150e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1830次成本:tensor(3.9864e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1840次成本:tensor(3.9482e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1850次成本:tensor(3.8910e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1860次成本:tensor(3.8338e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1870次成本:tensor(3.7861e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1880次成本:tensor(3.7193e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1890次成本:tensor(3.6812e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1900次成本:tensor(3.6144e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1910次成本:tensor(3.5477e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1920次成本:tensor(3.4809e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1930次成本:tensor(3.4809e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1940次成本:tensor(3.4237e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1950次成本:tensor(3.3951e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "第1960次成本:tensor(3.3569e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch  1965: reducing learning rate of group 0 to 1.0000e-04.\n",
      "第1970次成本:tensor(3.3474e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch  1977: reducing learning rate of group 0 to 1.0000e-05.\n",
      "第1980次成本:tensor(3.3474e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch  1988: reducing learning rate of group 0 to 1.0000e-06.\n",
      "第1990次成本:tensor(3.3474e-07, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "Epoch  1999: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2000):\n",
    "    output = net(train_x.cuda())\n",
    "    loss=loss_func(output,train_y.cuda()).cuda()\n",
    "    optimizer.zero_grad() \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step(loss)\n",
    "    if epoch % 10 == 0:\n",
    "        print('第'+str(epoch)+'次成本:'+str(loss))                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存成功\n"
     ]
    }
   ],
   "source": [
    "torch.save(net.state_dict(),'params_res_true_train2.pkl')\n",
    "print('保存成功')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存成功\n"
     ]
    }
   ],
   "source": [
    "torch.save(net.state_dict(),'params_res_true_train2_two.pkl')\n",
    "print('保存成功')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
